{"cells":[{"cell_type":"markdown","source":["# This notebook automates the deployment of Microsoft Fabric data pipelines in another Fabric tenant\n","----------------------------------------------------------------------------------------------------------------\n","- **FabricDataPipelineAutoDeployment**\n","- **Version V1.0**\n","\n","- **PREREQUISITES**\n","- Please ensure that the **JSON files & YAML files** are kept at the lakehouse folder.\n","- Ensure the YAML configuration values are updated.\n","----------------------------------------------------------------------------------------------------------------\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26a57d92-ad2f-44bc-9fa3-089f3343dbf7"},{"cell_type":"markdown","source":["# Install jsonpath_ng library\n","- jsonpath_ng library provides functionality for querying JSON-like data structures using JSONPath expressions. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"00239377-3c68-4bd1-9c83-a7731bde1592"},{"cell_type":"code","source":["%pip install jsonpath_ng"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"286fc8b1-51d8-457c-835e-f7b8c6e712d7"},{"cell_type":"markdown","source":["# Import statements for necessary libraries:\n","\n","- `requests`: Used for making HTTP requests.\n","- `json`: Provides functions for encoding and decoding JSON data.\n","- `base64`: Used for encoding and decoding data in Base64 format.\n","- `sempy` library and its `fabric` module for working with Fabric in Sempy.\n","- `pyyaml` library is used for YAML parsing and emitter for Python."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a1543088-ea0e-4501-81b6-6db545b7969a"},{"cell_type":"code","source":["# import libraries\n","import requests\n","import json\n","import base64\n","import sempy\n","import sempy.fabric as fabric\n","import yaml"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"edc9f89e-64ef-4edd-bb57-04e88b0d591b"},{"cell_type":"markdown","source":["# Get current workspace id & session token"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8928b777-0463-450d-a6ea-dc15895c21a0"},{"cell_type":"code","source":["workspace_id = fabric.get_workspace_id()\n","access_token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com/\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0f91a4f-d875-490f-a88a-342667323aa9"},{"cell_type":"markdown","source":["# Read and parse the configuration file"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a0f8b6f2-0fee-475b-b079-ae9793a040a1"},{"cell_type":"code","source":["base_folder_path = \"/lakehouse/default/Files/\"  # modify based on your environment e.g. Files/ in Fabric\n","config_file_name = \"pipeline_deployment_config.yml\"\n","config_file_path = base_folder_path + config_file_name\n","config_data = None\n","externalreference_id_map = {}  # Create a map to store oldconnectionid and newconnectionid\n","\n","# Get details of data warehouse name, id & endpoint from configuration file\n","dest_dw_name = \"\"\n","dest_dw_object_id = \"\"\n","dest_dw_endpoint = \"\"\n","\n","# Get details of lakehouse name, id & endpoint from configuration file\n","dest_lh_name = \"\"\n","dest_lh_object_id = \"\"\n","dest_lh_endpoint = \"\"\n","\n","# Read the config file\n","try:\n","    with open(config_file_path, 'r') as file:\n","        config_data = yaml.safe_load(file)\n","        # Debug: Print the type and content of config_data\n","        # print(f\"Loaded config data (type: {type(config_data)}): {config_data}\")\n","except yaml.YAMLError as exc:\n","    print(f\"Error reading YAML file: {exc}\")\n","except FileNotFoundError as fnf_error:\n","    print(f\"Config file not found: {fnf_error}\")\n","\n","# Process the config data if available\n","if isinstance(config_data, dict):\n","    # Process pipeline configuration files\n","    pipeline_config_files = config_data.get(\"pipelinejsonfiles\", [])\n","\n","    # Read original pipeline reference IDs\n","    original_pipeline_ids = {}\n","    for item in config_data.get('originalpipelinereferenceids', []):\n","        display_name = item.get('pipelinedisplayname')\n","        pipeline_id = item.get('originalpipelineid')\n","        original_pipeline_ids[display_name] = pipeline_id\n","\n","    # Process Destination Datawarehouse details\n","    datawarehouse_details = config_data.get(\"DestinationDatawarehousedetails\", [])\n","    for item in datawarehouse_details:\n","        dest_dw_name = item.get(\"name\")\n","        dest_dw_object_id = item.get(\"objectId\")\n","        dest_dw_endpoint = item.get(\"endpoint\")\n","\n","    # Process Destination Lakehouse details\n","    lakehouse_details = config_data.get(\"DestinationLakehousedetails\", [])\n","    for item in lakehouse_details:\n","        dest_lh_name = item.get(\"name\")\n","        dest_lh_object_id = item.get(\"objectId\")\n","        dest_lh_endpoint = item.get(\"endpoint\")\n","    for pipeline in pipeline_config_files:\n","        external_refs = pipeline.get('externalreferences')\n","        if external_refs:  # Check if external references exist\n","            for ref in external_refs:\n","                old_id = ref.get('oldconnectionid')\n","                new_id = ref.get('newconnectionid')\n","                if old_id and new_id:  # Ensure both IDs are present\n","                    externalreference_id_map[old_id] = new_id\n","else:\n","    print(\"No valid configuration data found.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c5f4e11a-dcae-4c12-bc24-0224a64a0e2b"},{"cell_type":"markdown","source":["# Function to encode input string to base64"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"94ceda4a-276d-4991-96e7-be69a46e8304"},{"cell_type":"code","source":["def b64EncodeString(json_str):\n","    # Encode the string using Base64\n","    base64_encoded = base64.b64encode(json_str.encode('utf-8'))\n","    base64_encoded_str = base64_encoded.decode('utf-8')\n","    return base64_encoded_str"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c2fc8c4c-509a-4367-a524-1a602a57966d"},{"cell_type":"markdown","source":["# Read JSON file from Lakehouse location"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"849bc148-8967-4b6a-b6f8-9c7ca45a0250"},{"cell_type":"code","source":["def readJsonFile(file_path):\n","    try:\n","        with open(file_path, 'r') as file:\n","            json_data = json.load(file)\n","            # Remove 'name' and 'objectId' properties\n","            if 'name' in json_data:\n","                del json_data['name']\n","            if 'objectId' in json_data:\n","                del json_data['objectId']\n","            return json_data\n","    except FileNotFoundError:\n","        print(f\"File '{file_path}' not found.\")\n","    except json.JSONDecodeError:\n","        print(f\"Error decoding JSON in file '{file_path}'.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d61e3f35-2267-42eb-a839-daf021d80d2b"},{"cell_type":"markdown","source":["# Read & Replace Embedded PipelineIDs Json"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"265559b5-0674-4b71-a601-81a6d469a106"},{"cell_type":"code","source":["def readandReplaceEmbeddedPipelineIDsJson(filepath,invoked_pipeline_list,successfully_created_pipelines):\n","    try:\n","        with open(file_path, 'r') as file:\n","            json_data = json.load(file)\n","            for invoked_pipeline in invoked_pipeline_list:\n","                new_pipeline_id = successfully_created_pipelines.get(invoked_pipeline)\n","                original_pipeline_id = original_pipeline_ids.get(invoked_pipeline)\n","                json_data = replace_property(json_data,original_pipeline_id,new_pipeline_id)\n","            return json_data\n","    except FileNotFoundError:\n","        print(f\"File '{file_path}' not found.\")\n","    except json.JSONDecodeError:\n","        print(f\"Error decoding JSON in file '{file_path}'.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"91fa317a-1e11-4575-97d7-17414c56a510"},{"cell_type":"markdown","source":["# Replace individual property.\n","- Sub function used by readandReplaceEmbeddedPipelineIDsJson"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29d974ff-f325-4b43-a618-ecf4f26a162a"},{"cell_type":"code","source":["def replace_property(json_obj, old_value, new_value):\n","    if isinstance(json_obj, dict):\n","        for key, value in json_obj.items():\n","            if isinstance(value, (dict, list)):\n","                replace_property(value, old_value, new_value)\n","            elif value == old_value:\n","                json_obj[key] = new_value\n","    elif isinstance(json_obj, list):\n","        for item in json_obj:\n","            replace_property(item, old_value, new_value)\n","    return json_obj"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"66904036-7869-4532-99dc-06895d529dcd"},{"cell_type":"markdown","source":["# Replace external references connection value"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d06d7220-7b77-4d40-b1c4-d1a04653f11b"},{"cell_type":"code","source":["def replace_externalreferences_connection_value(json_data, old_value, new_value):\n","    try:\n","        json_str = json.dumps(json_data)\n","        updated_json_str = json_str.replace(old_value, new_value)\n","        return json.loads(updated_json_str)\n","    except Exception as e:\n","        print(f\"Replace error for external references {e}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8cfc0aa9-e11b-43b6-87fe-c5db64d7032c"},{"cell_type":"markdown","source":["# Replace linked service details"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34cdbc02-f484-4e5b-ab4a-2b798746571e"},{"cell_type":"code","source":["from jsonpath_ng import parse \n","def replace_linked_service_details(json_data):\n","    jsonpath_expr = parse('$..linkedService')\n","    matches = jsonpath_expr.find(json_data)\n","    \n","    if matches:\n","        for match in matches:\n","            linked_service = match.value\n","            if(linked_service['properties']['type'] == 'DataWarehouse'):\n","                linked_service['name'] = dest_dw_name\n","                linked_service['objectId'] = dest_dw_object_id\n","                linked_service['properties']['typeProperties']['artifactId'] = dest_dw_object_id\n","                linked_service['properties']['typeProperties']['endpoint'] = dest_dw_endpoint\n","            elif(linked_service['properties']['type'] == 'Lakehouse'):\n","                linked_service['name'] = dest_lh_name\n","                linked_service['properties']['typeProperties']['artifactId'] = dest_lh_object_id\n","                linked_service['properties']['typeProperties']['workspaceId'] = workspace_id\n","    else:\n","        print(f\"No matches found\")\n","    return json_data"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6445b561-63fa-48ab-9f89-7459c478fd2a"},{"cell_type":"markdown","source":["# Deploy Fabric Data Pipelines"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08126d68-063e-4524-b447-a61a07794f23"},{"cell_type":"code","source":["url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\"\n","\n","successfully_created_pipelines = dict()\n","\n","for item in pipeline_config_files:\n","  display_name = item.get(\"pipelinedisplayname\")\n","  file_path = item.get(\"pipelinejsonfilepath\")\n","  invoke_pipeline = item.get(\"invokepipeline\")\n","  externalreferencesflg = item.get(\"externalreferencesflg\")\n","  internalreferencesflg = item.get(\"internalreferencesflg\")\n","  invoked_pipeline_list = item.get(\"invokedpipelinelist\", []) if invoke_pipeline == \"true\" else None \n","\n","  if invoke_pipeline == \"false\":\n","    json_file_value =  readJsonFile(file_path)\n","  elif invoke_pipeline == \"true\":\n","    json_file_value =  readandReplaceEmbeddedPipelineIDsJson(file_path,invoked_pipeline_list,successfully_created_pipelines)\n","\n","  if internalreferencesflg == \"true\":\n","    json_file_value = replace_linked_service_details(json_file_value)\n","\n","  if externalreferencesflg == \"true\":\n","    for key, value in externalreference_id_map.items():\n","      json_file_value = replace_externalreferences_connection_value (json_file_value,key,value)\n","\n","  # Convert JSON object to string\n","  json_str = json.dumps(json_file_value)\n","\n","  base64_encoded_str = b64EncodeString(json_str)\n","\n","  payload = json.dumps({\n","    \"displayName\": display_name,\n","    \"type\": \"DataPipeline\",\n","    \"definition\": {\n","      \"parts\": [\n","        {\n","          \"path\": \"pipeline-content.json\",\n","          \"payload\": base64_encoded_str,\n","          \"payloadType\": \"InlineBase64\"\n","        }\n","      ]\n","    }\n","  })\n","  headers = {\n","    'Content-Type': 'application/json',\n","    'Authorization': f'Bearer {access_token}'\n","  }\n","  response = requests.request(\"POST\", url, headers=headers, data=payload)\n","  #print(response.text)\n","  response_dict = json.loads(response.text)\n","\n","  if response.ok:\n","    print(f\">> Pipeline '{display_name}' deployed.\")\n","  else:\n","    raise RuntimeError(f\"Pipeline '{display_name}' creation failed: {response.status_code}: {response.text}\")\n","\n","  if \"id\" in response_dict:\n","    successfully_created_pipelines[response_dict['displayName']]= response_dict['id']"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ac493be4-42fb-48b7-9c9f-94aff35d623f"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"a1e8b2fe-7527-40b7-a137-648be36a6602","known_lakehouses":[{"id":"e184ce0d-ea7b-4a87-8e1c-baf9ba82d5a6"},{"id":"a1e8b2fe-7527-40b7-a137-648be36a6602"}],"default_lakehouse_name":"CMFLHStore","default_lakehouse_workspace_id":"66a92280-b91b-408e-bdd5-0276ad7d35a1"}}},"nbformat":4,"nbformat_minor":5}